{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Models for ObservationScheme Using PyCaret\n",
    "## Subsample: 100.000 of random concatenated text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics\n",
    "import seaborn as sns\n",
    "from pycaret.classification import *\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/NewData_ranCombination/Subsamples/Embeddings/LaBSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = [f for f in listdir(data_dir) if isfile(join(data_dir, f))]\n",
    "all_files = [f[:-4] for f in all_files]\n",
    "label_files = sorted([f for f in all_files if ('label' in f)])       # alphabetically ordered\n",
    "filenames = sorted([f for f in all_files if (f not in label_files)]) # alphabetically ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(np.load(f'{data_dir}/{filenames[0]}.npy'))\n",
    "print('Shape of data:', data.shape)\n",
    "\n",
    "train = data.iloc[:-2000,:]\n",
    "test = data.iloc[-2000:,:]\n",
    "\n",
    "print('Train:', train.shape)\n",
    "print('Test:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "labels = pd.DataFrame(np.load(f'{data_dir}/{label_files[0]}.npy'))\n",
    "train['obs_scheme'] = labels.iloc[:-2000,:]\n",
    "test['obs_scheme'] = labels.iloc[-2000:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\", train.obs_scheme.nunique())\n",
    "print(\"Test: \", test.obs_scheme.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyCaret Setup\n",
    "\n",
    "The setup() function of PyCaret initializes the environment and prepares the machine learning modeling data and deployment. There are two necessary parameters, a dataset, and the target variable. After executing the function, each feature's type is inferred, and several pre-processing tasks are performed on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = setup(\n",
    "    data = train,\n",
    "    test_data = test,\n",
    "    target = 'obs_scheme',\n",
    "    silent=True,\n",
    "    session_id = 1221)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = create_model('ridge', cross_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = create_model('lda', cross_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = create_model('lr', cross_validation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(\n",
    "    C=1.0,\n",
    "    class_weight=None,\n",
    "    dual=False,\n",
    "    fit_intercept = True,\n",
    "    intercept_scaling=1,\n",
    "    l1_ratio=None,\n",
    "    max_iter=1000,\n",
    "    multi_class='auto',\n",
    "    n_jobs=None,\n",
    "    penalty='l2',\n",
    "    random_state=1221,\n",
    "    solver='lbfgs',\n",
    "    tol=0.0001,\n",
    "    verbose=0,\n",
    "    warm_start=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR.fit(train.iloc[:,:-1], train.obs_scheme)\n",
    "y_pred = LR.predict(test.iloc[:,:-1])\n",
    "print(\"Accuracy LR:\", accuracy_score(test.obs_scheme, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'LR_labse_102000_ranCombination_sklearn.sav'\n",
    "pickle.dump(LR, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply on unseen test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_lr = predict_model(t_lr, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(\"LR:\", accuracy_score(ypred_lr.obs_scheme, ypred_lr.Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = metrics.confusion_matrix(test['obs_scheme'], ypred_lr['Label'])\n",
    "cm\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize = (20,10))\n",
    "sns.heatmap(pd.DataFrame(cm), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "save_model(t_lr, 'LR_labse_100000_ranCombination')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "520d1db8b5d7db4024068cda840687fc5229e2f4f4b5f1a68a03a2419364530e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('test2': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
